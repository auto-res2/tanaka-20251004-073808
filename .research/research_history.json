{
  "research_topic": "Graph Attention Networkの学習の高速化",
  "queries": [
    "efficient GAT training",
    "fast Graph Attention Network"
  ],
  "research_study_list": [
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      }
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      }
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      }
    }
  ]
}